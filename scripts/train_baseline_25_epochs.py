#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ Baseline CNN –Ω–∞ 25 —ç–ø–æ—Ö–∞—Ö
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import time
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from models.simply_cnn import SimpleCNN, count_parameters

def train_baseline_25_epochs():
    """–û–±—É—á–µ–Ω–∏–µ Baseline CNN –Ω–∞ 25 —ç–ø–æ—Ö–∞—Ö"""
    
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï BASELINE CNN –ù–ê 25 –≠–ü–û–•–ê–•")
    print("=" * 60)
    
    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = SimpleCNN(num_classes=10).to(device)
    total_params = count_parameters(model)
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {total_params:,}")
    
    # –°–æ–∑–¥–∞–µ–º DataLoader
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform
    )
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform
    )
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)
    
    print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –±–∞—Ç—á–µ–π: {len(train_loader)}")
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π: {len(test_loader)}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º LR –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)  # –°–Ω–∏–∂–∞–µ–º LR –∫–∞–∂–¥—ã–µ 8 —ç–ø–æ—Ö
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    
    # –û–±—É—á–µ–Ω–∏–µ
    start_time = time.time()
    
    for epoch in range(1, 26):  # 25 —ç–ø–æ—Ö
        print(f"\n–≠–ø–æ—Ö–∞ {epoch}/25:")
        
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
        
        train_loss = running_loss / len(train_loader)
        train_acc = 100. * correct / total
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                
                output = model(data)
                loss = criterion(output, target)
                
                running_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        val_loss = running_loss / len(test_loader)
        val_acc = 100. * correct / total
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
        
        print(f"  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
        print(f"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%")
        print(f"  LR:    {current_lr:.6f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if epoch == 1 or val_acc > max(val_accuracies[:-1]):
            best_model_state = model.state_dict().copy()
            best_epoch = epoch
            best_acc = val_acc
    
    total_time = time.time() - start_time
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ BASELINE CNN (25 –≠–ü–û–•):")
    print(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(val_accuracies):.2f}% (—ç–ø–æ—Ö–∞ {best_epoch})")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {val_accuracies[-1]:.2f}%")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è loss: {val_losses[-1]:.4f}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs('results/25_epochs', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_path = 'models/baseline_25epochs_model.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'best_model_state_dict': best_model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': 25,
        'best_epoch': best_epoch,
        'val_accuracy': val_accuracies[-1],
        'best_val_accuracy': max(val_accuracies),
        'val_loss': val_losses[-1],
        'total_params': total_params
    }, model_path)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ Loss
    epochs = list(range(1, 26))
    ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2, marker='o', markersize=3)
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=3)
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('Loss')
    ax1.set_title('Baseline CNN (25 epochs): Loss –ø–æ —ç–ø–æ—Ö–∞–º')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ Accuracy
    ax2.plot(epochs, train_accuracies, 'b-', label='Train Accuracy', linewidth=2, marker='o', markersize=3)
    ax2.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=3)
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Baseline CNN (25 epochs): Accuracy –ø–æ —ç–ø–æ—Ö–∞–º')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/25_epochs/baseline_25epochs_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results/25_epochs/baseline_25epochs_results.png")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    print(f"\nüìù –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...")
    
    filename = 'results/25_epochs/baseline_25epochs_report.txt'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("–û–¢–ß–ï–¢ –û–ë–£–ß–ï–ù–ò–Ø BASELINE CNN –ù–ê 25 –≠–ü–û–•–ê–•\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"–ú–æ–¥–µ–ª—å: Baseline CNN (25 epochs)\n")
        f.write(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}\n")
        f.write(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç\n")
        f.write(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(val_accuracies):.2f}% (—ç–ø–æ—Ö–∞ {best_epoch})\n")
        f.write(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {val_accuracies[-1]:.2f}%\n")
        f.write(f"–§–∏–Ω–∞–ª—å–Ω–∞—è loss: {val_losses[-1]:.4f}\n\n")
        
        f.write("–î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
        f.write("–≠–ø–æ—Ö–∞ | Train Loss | Train Acc | Val Loss | Val Acc\n")
        f.write("-" * 50 + "\n")
        
        for i, epoch in enumerate(epochs):
            f.write(f"{epoch:5d} | {train_losses[i]:9.4f} | "
                   f"{train_accuracies[i]:8.2f}% | "
                   f"{val_losses[i]:8.4f} | "
                   f"{val_accuracies[i]:7.2f}%\n")
    
    print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
    
    print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"–°–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã:")
    print(f"  - {model_path} (–º–æ–¥–µ–ª—å)")
    print(f"  - results/25_epochs/baseline_25epochs_results.png (–≥—Ä–∞—Ñ–∏–∫–∏)")
    print(f"  - {filename} (–æ—Ç—á–µ—Ç)")

if __name__ == "__main__":
    train_baseline_25_epochs()
