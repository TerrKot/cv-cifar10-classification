#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è confusion matrix –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from models.simply_cnn import SimpleCNN
from models.improve_cnn import ImprovedCNN

def load_model(model_path, model_type, device):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
    if model_type == 'baseline':
        model = SimpleCNN(num_classes=10)
    elif model_type == 'improved':
        model = ImprovedCNN(num_classes=10, dropout_rate=0.3)
    else:
        raise ValueError("model_type –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'baseline' –∏–ª–∏ 'improved'")
    
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    return model

def generate_confusion_matrix(model, test_loader, device, model_name, save_path):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è confusion matrix"""
    
    print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è confusion matrix –¥–ª—è {model_name}...")
    
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs, 1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(target.cpu().numpy())
    
    # –°–æ–∑–¥–∞–µ–º confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    
    # –ö–ª–∞—Å—Å—ã CIFAR-10
    classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 
              'dog', 'frog', 'horse', 'ship', 'truck')
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    plt.figure(figsize=(12, 10))
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # –°–æ–∑–¥–∞–µ–º heatmap
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    
    plt.title(f'Confusion Matrix - {model_name}', fontsize=16, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    accuracy = np.trace(cm) / np.sum(cm)
    
    # Precision, Recall, F1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    precision = np.diag(cm) / np.sum(cm, axis=0)
    recall = np.diag(cm) / np.sum(cm, axis=1)
    f1 = 2 * (precision * recall) / (precision + recall)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    report_path = save_path.replace('.png', '_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ - {model_name}\n")
        f.write("=" * 50 + "\n\n")
        
        f.write(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f} ({accuracy*100:.2f}%)\n\n")
        
        f.write("–ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú:\n")
        f.write("-" * 50 + "\n")
        f.write(f"{'–ö–ª–∞—Å—Å':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\n")
        f.write("-" * 50 + "\n")
        
        for i, class_name in enumerate(classes):
            f.write(f"{class_name:<12} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f}\n")
        
        f.write(f"\n–°–†–ï–î–ù–ò–ï –ú–ï–¢–†–ò–ö–ò:\n")
        f.write("-" * 30 + "\n")
        f.write(f"–°—Ä–µ–¥–Ω—è—è Precision: {np.mean(precision):.4f}\n")
        f.write(f"–°—Ä–µ–¥–Ω—è—è Recall: {np.mean(recall):.4f}\n")
        f.write(f"–°—Ä–µ–¥–Ω—è—è F1-Score: {np.mean(f1):.4f}\n")
    
    print(f"‚úÖ Confusion matrix —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}")
    print(f"‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_path}")
    
    return cm, accuracy, precision, recall, f1

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üìä –ì–ï–ù–ï–†–ê–¶–ò–Ø CONFUSION MATRIX –î–õ–Ø –í–°–ï–• –ú–û–î–ï–õ–ï–ô")
    print("=" * 60)
    
    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º DataLoader
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform
    )
    
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs('results/confusion_matrices', exist_ok=True)
    
    # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    models_to_analyze = [
        ('models/baseline_5epochs_model.pth', 'baseline', 'Baseline CNN (5 —ç–ø–æ—Ö)'),
        ('models/improved_5epochs_model.pth', 'improved', 'Improved CNN (5 —ç–ø–æ—Ö)'),
        ('models/baseline_25epochs_model.pth', 'baseline', 'Baseline CNN (25 —ç–ø–æ—Ö)'),
        ('models/improved_25epochs_model.pth', 'improved', 'Improved CNN (25 —ç–ø–æ—Ö)')
    ]
    
    results = {}
    
    for model_path, model_type, model_name in models_to_analyze:
        if os.path.exists(model_path):
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                model = load_model(model_path, model_type, device)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º confusion matrix
                save_path = f'results/confusion_matrices/{model_name.replace(" ", "_").replace("(", "").replace(")", "").lower()}_confusion_matrix.png'
                cm, accuracy, precision, recall, f1 = generate_confusion_matrix(
                    model, test_loader, device, model_name, save_path
                )
                
                results[model_name] = {
                    'accuracy': accuracy,
                    'precision': np.mean(precision),
                    'recall': np.mean(recall),
                    'f1': np.mean(f1),
                    'confusion_matrix': cm
                }
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {model_name}: {e}")
        else:
            print(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    if results:
        create_comparison_report(results)
    
    print(f"\nüéâ –ì–ï–ù–ï–†–ê–¶–ò–Ø CONFUSION MATRIX –ó–ê–í–ï–†–®–ï–ù–ê!")

def create_comparison_report(results):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    
    filename = 'results/confusion_matrices/comparison_report.txt'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –û–¢–ß–ï–¢ CONFUSION MATRIX\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("–ú–ï–¢–†–ò–ö–ò –ü–û –ú–û–î–ï–õ–Ø–ú:\n")
        f.write("-" * 50 + "\n")
        f.write(f"{'–ú–æ–¥–µ–ª—å':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\n")
        f.write("-" * 50 + "\n")
        
        for model_name, metrics in results.items():
            f.write(f"{model_name:<25} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} "
                   f"{metrics['recall']:<10.4f} {metrics['f1']:<10.4f}\n")
        
        f.write(f"\n–õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
        f.write("-" * 30 + "\n")
        
        best_accuracy = max(results.items(), key=lambda x: x[1]['accuracy'])
        best_precision = max(results.items(), key=lambda x: x[1]['precision'])
        best_recall = max(results.items(), key=lambda x: x[1]['recall'])
        best_f1 = max(results.items(), key=lambda x: x[1]['f1'])
        
        f.write(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy[0]} ({best_accuracy[1]['accuracy']:.4f})\n")
        f.write(f"–õ—É—á—à–∞—è precision: {best_precision[0]} ({best_precision[1]['precision']:.4f})\n")
        f.write(f"–õ—É—á—à–∏–π recall: {best_recall[0]} ({best_recall[1]['recall']:.4f})\n")
        f.write(f"–õ—É—á—à–∏–π F1-Score: {best_f1[0]} ({best_f1[1]['f1']:.4f})\n")
    
    print(f"‚úÖ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")

if __name__ == "__main__":
    main()
