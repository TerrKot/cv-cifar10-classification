#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è CNN –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ CIFAR-10
–í–∫–ª—é—á–∞–µ—Ç:
- Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
- Batch Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
- Data Augmentation –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import time
import os
import sys

class ImprovedCNN(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è CNN –¥–ª—è CIFAR-10 —Å Dropout –∏ BatchNorm"""
    
    def __init__(self, num_classes=10, dropout_rate=0.5):
        super(ImprovedCNN, self).__init__()
        
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ —Å BatchNorm –∏ Dropout
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(512)
        
        # Pooling
        self.pool = nn.MaxPool2d(2, 2)
        
        # Dropout
        self.dropout = nn.Dropout(dropout_rate)
        self.dropout2d = nn.Dropout2d(0.2)  # Spatial dropout
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.fc1 = nn.Linear(512 * 2 * 2, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, num_classes)
        
        # –ê–∫—Ç–∏–≤–∞—Ü–∏—è
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫: Conv -> BN -> ReLU -> Pool -> Dropout2D
        x = self.pool(self.relu(self.bn1(self.conv1(x))))
        x = self.dropout2d(x)
        
        # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫: Conv -> BN -> ReLU -> Pool -> Dropout2D
        x = self.pool(self.relu(self.bn2(self.conv2(x))))
        x = self.dropout2d(x)
        
        # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫: Conv -> BN -> ReLU -> Pool -> Dropout2D
        x = self.pool(self.relu(self.bn3(self.conv3(x))))
        x = self.dropout2d(x)
        
        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –±–ª–æ–∫: Conv -> BN -> ReLU -> Pool -> Dropout2D
        x = self.pool(self.relu(self.bn4(self.conv4(x))))
        x = self.dropout2d(x)
        
        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –≤–µ–∫—Ç–æ—Ä
        x = x.view(x.size(0), -1)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ —Å Dropout
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.dropout(self.relu(self.fc2(x)))
        x = self.fc3(x)
        
        return x

def get_augmented_data_loaders(batch_size=64, shuffle=True):
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader —Å Data Augmentation –¥–ª—è CIFAR-10"""
    
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),  # –°–ª—É—á–∞–π–Ω–æ–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ
        transforms.RandomRotation(10),            # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–≤–æ—Ä–æ—Ç –Ω–∞ ¬±10 –≥—Ä–∞–¥—É—Å–æ–≤
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # –°–ª—É—á–∞–π–Ω—ã–π —Å–¥–≤–∏–≥
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    ])
    
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
    ])
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=train_transform
    )
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=test_transform
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2
    )
    
    return train_loader, test_loader

def count_parameters(model):
    """–ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

class TrainingLogger:
    """–ö–ª–∞—Å—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, model_name="ImprovedCNN"):
        self.model_name = model_name
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
        self.epochs = []
        
    def log_epoch(self, epoch, train_loss, train_acc, val_loss, val_acc):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–æ—Ö–∏"""
        self.epochs.append(epoch)
        self.train_losses.append(train_loss)
        self.train_accuracies.append(train_acc)
        self.val_losses.append(val_loss)
        self.val_accuracies.append(val_acc)
        
    def save_logs(self, filename='improved_training_logs.txt'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –≤ —Ñ–∞–π–ª"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"–õ–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø {self.model_name.upper()}\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"–≠–ø–æ—Ö: {len(self.epochs)}\n")
            f.write(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {self.val_accuracies[-1]:.2f}%\n")
            f.write(f"–§–∏–Ω–∞–ª—å–Ω–∞—è loss –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {self.val_losses[-1]:.4f}\n\n")
            
            f.write("–î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
            f.write("–≠–ø–æ—Ö–∞ | Train Loss | Train Acc | Val Loss | Val Acc\n")
            f.write("-" * 50 + "\n")
            
            for i, epoch in enumerate(self.epochs):
                f.write(f"{epoch:5d} | {self.train_losses[i]:9.4f} | {self.train_accuracies[i]:8.2f}% | "
                       f"{self.val_losses[i]:8.4f} | {self.val_accuracies[i]:7.2f}%\n")
        
        print(f"‚úÖ –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

def train_epoch(model, train_loader, criterion, optimizer, device):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        optimizer.zero_grad()
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        output = model(data)
        loss = criterion(output, target)
        
        # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
        loss.backward()
        optimizer.step()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        running_loss += loss.item()
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 –±–∞—Ç—á–µ–π
        if batch_idx % 100 == 0:
            print(f'  –ë–∞—Ç—á {batch_idx}/{len(train_loader)}: '
                  f'Loss: {loss.item():.4f}, '
                  f'Acc: {100. * correct / total:.2f}%')
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc

def validate_epoch(model, val_loader, criterion, device):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            
            output = model(data)
            loss = criterion(output, target)
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    epoch_loss = running_loss / len(val_loader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc

def plot_training_results(logger, save_path='improved_training_results.png'):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ Loss
    ax1.plot(logger.epochs, logger.train_losses, 'b-', label='Train Loss', linewidth=2)
    ax1.plot(logger.epochs, logger.val_losses, 'r-', label='Validation Loss', linewidth=2)
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('Loss')
    ax1.set_title(f'{logger.model_name}: Loss –ø–æ —ç–ø–æ—Ö–∞–º')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ Accuracy
    ax2.plot(logger.epochs, logger.train_accuracies, 'b-', label='Train Accuracy', linewidth=2)
    ax2.plot(logger.epochs, logger.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title(f'{logger.model_name}: Accuracy –ø–æ —ç–ø–æ—Ö–∞–º')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")

def train_improved_model(epochs=5, learning_rate=0.001, batch_size=64, dropout_rate=0.5):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô CNN –î–õ–Ø CIFAR-10")
    print("=" * 60)
    print("–£–ª—É—á—à–µ–Ω–∏—è:")
    print("  ‚úì Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏")
    print("  ‚úì Batch Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏")
    print("  ‚úì Data Augmentation –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö")
    print("  ‚úì –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞")
    print("=" * 60)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = ImprovedCNN(num_classes=10, dropout_rate=dropout_rate).to(device)
    total_params = count_parameters(model)
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {total_params:,}")
    
    # –°–æ–∑–¥–∞–µ–º DataLoader —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
    print(f"\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å Data Augmentation...")
    train_loader, val_loader = get_augmented_data_loaders(batch_size=batch_size)
    print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –±–∞—Ç—á–µ–π: {len(train_loader)}")
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π: {len(val_loader)}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.7)
    
    print(f"\n‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò –û–ë–£–ß–ï–ù–ò–Ø:")
    print(f"–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: CrossEntropyLoss")
    print(f"–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam (lr={learning_rate}, weight_decay=1e-4)")
    print(f"Scheduler: StepLR (step_size=2, gamma=0.7)")
    print(f"–≠–ø–æ—Ö: {epochs}")
    print(f"Batch size: {batch_size}")
    print(f"Dropout rate: {dropout_rate}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
    logger = TrainingLogger("ImprovedCNN")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print(f"\nüéØ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø:")
    print("-" * 60)
    
    start_time = time.time()
    
    for epoch in range(1, epochs + 1):
        epoch_start = time.time()
        
        print(f"\n–≠–ø–æ—Ö–∞ {epoch}/{epochs}:")
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.log_epoch(epoch, train_loss, train_acc, val_loss, val_acc)
        
        epoch_time = time.time() - epoch_start
        
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏ {epoch}:")
        print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        print(f"  Learning Rate: {current_lr:.6f}")
        print(f"  –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {epoch_time:.1f}—Å")
    
    total_time = time.time() - start_time
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìà –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {max(logger.val_accuracies):.2f}%")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {logger.val_accuracies[-1]:.2f}%")
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è loss –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {logger.val_losses[-1]:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤
    logger.save_logs('results/improved_training_logs.txt')
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plot_training_results(logger, 'results/improved_training_results.png')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_path = 'models/improved_model.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epochs,
        'val_accuracy': logger.val_accuracies[-1],
        'val_loss': logger.val_losses[-1],
        'total_params': total_params,
        'dropout_rate': dropout_rate,
        'learning_rate': learning_rate
    }, model_path)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    return model, logger

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    EPOCHS = 5
    LEARNING_RATE = 0.001
    BATCH_SIZE = 64
    DROPOUT_RATE = 0.5
    
    print("üéØ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –£–õ–£–ß–®–ï–ù–ù–û–ô CNN")
    print("=" * 50)
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    model, logger = train_improved_model(
        epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        batch_size=BATCH_SIZE,
        dropout_rate=DROPOUT_RATE
    )
    
    print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
    print(f"  - improved_training_logs.txt (–ª–æ–≥–∏)")
    print(f"  - improved_training_results.png (–≥—Ä–∞—Ñ–∏–∫–∏)")
    print(f"  - improved_model.pth (–º–æ–¥–µ–ª—å)")

if __name__ == "__main__":
    main()
